# 大模型本质——神经网络 #

这是一个很好的开篇，点明了当前大模型技术的核心基石。

1. 机械学习（通过数据训练模型，使系统自动学习并改进任务性能，无需显式编程）：
* 监督学习 (Supervised Learning)：有标签数据，模型学习输入到输出的映射关系。例如：图像分类（输入图片，输出类别标签）、回归预测（输入房屋特征，输出房价）。
* 无监督学习 (Unsupervised Learning)：无标签数据，模型学习数据本身的结构和模式。例如：聚类（将相似的数据点分到一组）、降维（PCA）。
* 强化学习 (Reinforcement Learning)：模型（智能体）通过与环境交互，根据获得的奖励或惩罚来学习最优策略。例如：AlphaGo下棋、机器人控制。

* **深度学习 (Deep Learning)**：是机器学习的一个分支，主要使用包含多个隐藏层的神经网络（即深度神经网络）。它在图像、语音、自然语言处理等领域取得了巨大成功。
* **生成式AI (Generative AI)**：专注于创建新的、原创内容的人工智能，如文本、图像、音频、视频等。大语言模型是生成式AI在文本领域的核心。
* **大语言模型 (Large Language Models, LLMs)**：通常指参数量巨大（数十亿甚至万亿级别）、在海量文本数据上预训练的深度学习模型，展现出强大的自然语言理解和生成能力。
2. 什么是模型：
* 基于输入数据通过假想函数得到输出函数: 这个描述很形象。更形式化地说，模型是一个函数（或一组函数）f_
theta(x)，其中 x 是输入数据，
theta 是模型的参数（权重和偏置），模型的目标是学习一个合适的 
theta，使得 f_
theta(x) 的输出 y 
′
  尽可能接近真实的输出 y（在监督学习中）或揭示数据的某种结构（在无监督学习中）。
* 数据集复杂——>这时候可以使用神经网络: 是的，神经网络尤其擅长处理高维、复杂的非线性数据关系，这是传统机器学习方法可能难以有效处理的。

3. 神经网络基础
* 抽象神经网络：MP模型 (McCulloch-Pitts Neuron Model)：这是最早的、非常简化的神经元模型，它接收二进制输入，并通过一个阈值函数产生二进制输出。它奠定了后续更复杂神经元模型的基础。
* 神经元越来越多，处理难度指数级的增长，如何优化？:
* 硬件层面: GPU、TPU等专用硬件的出现，提供了强大的并行计算能力。
* 算法层面:
* 更高效的优化算法: 如Adam、RMSprop等，不仅仅是梯度下降。
* 更有效的激活函数: 如ReLU及其变种，缓解了梯度消失问题。
* 网络结构创新: 如卷积神经网络 (CNN) 处理图像，循环神经网络 (RNN) 处理序列，以及后来更强大的Transformer。
* 正则化技术: 如Dropout、权重衰减，防止过拟合。
* 分布式训练: 将模型或数据分布到多个设备或机器上进行并行训练。
* 软件框架: TensorFlow, PyTorch等深度学习框架简化了模型构建和训练过程。

4. 模型训练：
* 预测与损失值，梯度优化，反向传播: 这是监督学习中训练神经网络的核心流程。
* 核心流程：定义网络结构，初始化参数，向前传播得到预测值，计算损失，反向传播得到梯度，更新参数。 (您漏了“计算损失”和“更新参数”这两个关键步骤)
* 向前传播 (Forward Propagation)：输入数据通过网络，逐层计算，最终得到模型的预测输出。
* 计算损失 (Loss Calculation)：将模型的预测输出与真实标签（在监督学习中）进行比较，通过损失函数（Loss Function，如交叉熵损失、均方误差损失）计算出两者之间的差异，即损失值。
* 反向传播 (Backward Propagation)：根据损失值，利用链式法则从输出层开始逐层计算每个参数对于损失的梯度（偏导数）。
* 参数更新 (Parameter Update)：使用优化算法（如梯度下降）根据计算出的梯度来调整模型的参数（权重和偏置），以减小损失。

5. 梯度下降：优化算法，还有许多变种
* 是的，梯度下降是最基础的优化算法。
* 常见变种:
* 批量梯度下降 (BGD): 使用整个训练集的梯度来更新参数。计算精确但慢，且对内存要求高。
* 随机梯度下降 (SGD): 每次只用一个样本的梯度来更新参数。速度快，波动大，有助于跳出局部最优，但收敛过程不稳定。
* 小批量随机梯度下降 (Mini-batch SGD): 每次使用一小批样本的梯度来更新参数。是BGD和SGD的折中，兼顾了效率和稳定性，是目前最常用的方法。
* 常见的激活函数：
* Sigmoid: 将输出压缩到 (0, 1)，常用于二分类输出层或早期的神经网络，但易产生梯度消失。
* Tanh: 将输出压缩到 (-1, 1)，通常比Sigmoid表现好一些，但也有梯度消失问题。
* ReLU (Rectified Linear Unit): f(x)=max(0,x)。计算简单，收敛快，有效缓解梯度消失，是目前最常用的激活函数之一。
* Leaky ReLU, PReLU, ELU: ReLU的变种，试图解决ReLU在负数区间的“死亡”问题。
* Softmax: 常用于多分类问题的输出层，将输出转换为概率分布。
* 优化器：避免步长过大或过小:
* 优化器的作用不仅仅是调整步长（学习率），还包括如何利用梯度信息来更有效地更新参数。
* 常见的优化器: SGD with Momentum, AdaGrad, RMSprop, Adam (目前非常流行且效果通常较好)。
* 它们通过引入动量、自适应学习率等机制来加速收敛并改善训练的稳定性。
* 明确位置，找到方向，走，重复步骤直到模型收敛，训练完成: 这个比喻很好地描述了梯度下降的过程。“位置”是当前参数点，“方向”是负梯度方向，“走”是根据学习率更新参数。
* 输入数据来训练，随机梯度下降 全部：批量～ ，小撮：小批量～: 这个对应关系是：
* 全部数据 -> 批量梯度下降 (BGD)
* 单个数据 -> 随机梯度下降 (SGD)
* 小撮数据 -> 小批量随机梯度下降 (Mini-batch SGD)

大模型框架：transformer
* Transformer架构确实是当前大语言模型的基础。
* Tokenizer (分词器)：将原始文本分割成模型可以理解的单元（tokens），这些单元可能是词、子词（subword，如BPE, WordPiece, SentencePiece）。
* Embedding (嵌入层)：将每个token映射到一个高维稠密向量表示。这个向量捕捉了token的语义信息。
* Positional Encoding (位置编码)：由于Transformer的自注意力机制本身不包含序列顺序信息，位置编码向输入嵌入中添加了关于token在序列中位置的信息。
* Transformer Block (Transformer模块)：这是Transformer模型的核心重复单元，通常由以下部分组成：
* Multi-Head Attention (多头注意力机制)：这是关键。
* Add & Norm (残差连接与层归一化)：帮助训练更深的网络。
* Feedforward Network (前馈神经网络)：通常是一个两层的全连接网络，对注意力机制的输出进行进一步处理。
* Add & Norm：再次应用。
* Attention (注意力机制) / Self-Attention (自注意力机制)：允许模型在处理一个token时，动态地关注输入序列中的其他所有token，并根据相关性赋予不同的权重。模型可以“理解”哪些词对当前词的含义最重要。
* QVK (Query, Key, Value) 是自注意力的核心概念:
* Query (查询): 代表当前正在处理的token。
* Key (键): 代表序列中可以被查询的其他token。
* Value (值): 代表序列中其他token实际携带的信息。
* 通过计算Query与所有Key的相似度（通常是点积），得到注意力权重，然后用这些权重对相应的Value进行加权求和，得到当前Query的上下文感知表示。
* Q, K, V 通常是通过将输入嵌入（或上一层的输出）乘以不同的权重矩阵（这些矩阵是模型学习的参数）得到的。
* Feedforward Network (前馈网络)：在注意力层之后，每个位置的输出会独立地通过一个前馈网络。
* Softmax函数: 在多头注意力的最后一步（计算注意力权重时）和模型最终输出层（如果是分类任务，如预测下一个token）会用到，将数值转换为概率分布。

多头注意力模块（自注意力注意模型）qvk是个矩阵: 是的，Q, K, V 通常是矩阵，其中每一行对应序列中的一个token的向量表示。多头注意力允许模型在不同的“表示子空间”中并行地学习不同方面的注意力信息。
transformer变种：
1.  仅编码器/自编码器模型 (Encoder-only / Auto-encoding models): 例如 BERT, RoBERTa。它们通过编码器将输入序列转换为上下文相关的表示。非常适合自然语言理解 (NLU) 任务，如文本分类、情感分析、命名实体识别。它们通常使用“掩码语言模型 (Masked Language Model, MLM)”等目标进行预训练。
2.  仅解码器/自回归模型 (Decoder-only / Autoregressive models): 例如 GPT系列 (GPT-2, GPT-3, GPT-4), PaLM。它们从左到右生成文本，每个token的生成都依赖于之前已经生成的token。非常适合自然语言生成 (NLG) 任务，如文本生成、对话系统、机器翻译（作为其中一部分）。它们通常使用“下一个词预测 (Next Token Prediction)”作为预训练目标。
3.  序列到序列模型/编码器-解码器模型 (Sequence-to-sequence / Encoder-Decoder models): 例如原始Transformer论文中的模型, T5, BART。它们包含一个编码器来处理输入序列和一个解码器来生成输出序列。非常适合机器翻译、文本摘要、问答等需要将一种序列转换为另一种序列的任务。

如何训练一个gpt (这里指GPT系列这类Decoder-only模型)
1.  无监督预训练 (Unsupervised Pre-training):
* 通过大量的文本，进行无监督学习的预训练: 是的，这是核心。使用来自互联网、书籍、文章等海量、多样化的文本数据。
* 模型看见文本，基于上下文预测得到一个token，预测与正确答案损失计算，更新权重: 这是“下一个词预测”或“语言建模”任务。模型接收一段文本作为输入，目标是预测序列中的下一个词（token）。损失函数（通常是交叉熵损失）会比较模型预测的下一个词的概率分布与真实下一个词之间的差异，然后通过反向传播更新模型的权重。
2.  监督微调 (Supervised Fine-Tuning, SFT):
* 改变函数内部函数，适应特定任务: 更准确地说，是使用预训练好的模型权重作为起点，在一个特定任务相关的、带标签的小规模高质量数据集上继续训练模型，以使其更好地适应这个特定任务。例如，对于问答任务，数据集是 (问题, 答案) 对；对于指令遵循任务，数据集是 (指令, 期望的输出) 对。
* 使用少量高质数据，训练，得到sft: “少量”是相对于预训练数据而言，“高质”非常重要。SFT模型是第二阶段的模型。
3.  训练奖励模型 (Reward Model, RM) + 从人类反馈中强化学习 (Reinforcement Learning from Human Feedback, RLHF):
* ai回答，人进行打分，三h原则；，得到奖励模型:
* 首先，使用SFT模型（或多个模型）对一批提示（prompts）生成多个回答。
* 人类评估员对这些回答进行排序或打分，指出哪些回答更好，哪些更差。评估标准通常遵循“3H”原则：Helpful (有用的), Honest (诚实的), Harmless (无害的)。
* 利用这些人类偏好数据（例如，(prompt, chosen_answer, rejected_answer) 对）来训练一个奖励模型 (RM)。RM的目标是学习预测对于给定的prompt和response，人类会给出多高的“奖励分数”。
* 接下来进行强化学习:
* 将SFT模型作为强化学习的策略模型 (policy)。
* 让策略模型针对新的prompt生成回答。
* 使用训练好的奖励模型 (RM) 来评估策略模型生成的回答，并给出奖励分数。
* 使用强化学习算法（如 PPO - Proximal Policy Optimization，这是RLHF中常用的算法）根据奖励模型的反馈来更新策略模型（即SFT模型）的参数，目标是让策略模型生成能够获得更高奖励（即更符合人类偏好）的回答。

llm核心技术
* 预训练：transformer架构和自注意力机制，捕捉文本的依赖关系: 完全正确。
1.  SFT (Supervised Fine-Tuning): 如上所述。
2.  RM (Reward Modeling): 如上所述。
3.  RLHF (Reinforcement Learning from Human Feedback)，PPO: 如上所述。
4.  DPO (Direct Preference Optimization): 一种新兴的、替代传统两阶段RLHF（先训练RM，再用RM做RL）的方法。DPO直接使用人类偏好数据来优化语言模型，试图直接最大化生成更受偏好回答的概率，而无需显式训练一个独立的奖励模型。它被认为可能更简单、更稳定。
5.  PEFT (Parameter-Efficient Fine-Tuning)：参数高效微调。
* 涉及线代: 是的，很多PEFT方法都涉及到对模型权重矩阵的低秩分解、添加少量可训练参数等线性代数操作。
* 目的: 在微调大型预训练模型时，只更新模型参数的一小部分，而不是全部参数。
* 好处:
* 大大减少了微调所需的计算资源和存储空间。
* 可以为多个不同任务训练出多个“轻量级”的适配器 (adapters)，而共享同一个大型基础模型。
* 有时能缓解灾难性遗忘问题。
* 常见方法: LoRA (Low-Rank Adaptation), Prefix Tuning, P-Tuning, Prompt Tuning, Adapters等。

推理时显存计算：模型参数权重80%，输入输出数据，中间激活值，kv cache 等；:  
模型参数权重 (Model Parameters/Weights): 这是占用显存的大头，尤其是对于大模型。例如，一个7B参数的模型，如果用FP16（半精度浮点数，每个参数2字节）存储，就需要大约 14GB 的显存。这通常占显存的绝大部分，您说的80%是可能的。  
输入输出数据 (Input/Output Data): 输入的prompt和生成的文本在转换为token ID并嵌入为向量后，也会占用显存。  
中间激活值 (Intermediate Activations): 在前向传播过程中，每一层网络的输出（激活值）都需要存储起来，以便在反向传播时计算梯度（在训练时）。在推理时，如果不需要反向传播，很多中间激活值可以不保存，但某些结构（如残差连接）可能仍需保留上一层输出。然而，对于自回归生成模型（如GPT），在推理时，主要的中间状态存储是下面的KV Cache。   
KV Cache (Key-Value Cache): 在Transformer解码器（或仅解码器模型）的自注意力层中，当模型逐个生成token时，对于已经生成的token序列，它们的Key (K) 和 Value (V) 向量可以被缓存起来。这样，在生成下一个新token时，新token的Query (Q) 只需要与所有先前token的缓存K, V进行注意力计算，而不需要为之前的每个token重新计算K和V。这大大加速了自回归模型的推理速度，但KV Cache本身会随着生成序列长度的增加而占用越来越多的显存。这是推理时显存占用的一个重要部分，尤其是在生成长文本时。  
# ai应用 
Prompt  
系统角色 用户角色 助手角色  
RAG  
CoT思维链  
Agent (决策 感知 行动)  
这都是当前LLM应用中的热点和关键技术。下面进行一些补充和细化：  

Prompt Engineering (提示工程)  
您已经列出了一个很好的Prompt结构要素。它是与LLM有效交互的核心。除了角色扮演和任务拆解，还可以补充一些技巧：  

少样本提示 (Few-shot Prompting): 在Prompt中给模型提供几个具体的输入输出示例，引导模型理解任务并按期望格式输出。  

思维链 (Chain-of-Thought, CoT) - 您已提及:  

工作原理: 通过在提示中引导模型“一步一步地思考”或“展示推理过程”，来提升模型在复杂推理任务（如数学应用题、逻辑推理）上的表现。模型会先输出中间的推理步骤，然后再给出最终答案。  
扩展:  
Zero-shot CoT: 更简单，只需在提问后加上一句“让我们一步一步地思考”（Let's think step by step）。  
Self-Consistency: 生成多个不同的思维链路径，然后通过投票等方式选择最一致的答案，进一步提高准确率。  
Tree of Thoughts (ToT): 允许模型探索多个不同的推理路径（形成一棵思维树），并在每个步骤进行评估和选择，更接近人类解决复杂问题的方式。  
Graph of Thoughts (GoT): 将思维过程组织成更复杂的图结构，允许更灵活的思维路径组合与提炼。  
指令的清晰度和明确性: 避免模糊不清的指令，提供所有必要的上下文信息。
  
输出格式约束:明确要求模型按特定格式输出（如JSON, Markdown表格等）。  
  
RAG (Retrieval Augmented Generation - 检索增强生成)
您描述的“微调成本过高，或者把最新的知识进行外挂，如同给他一个书架”非常形象准确。
  
核心流程:  
检索 (Retrieval): 当用户提出问题时，首先使用用户的问题（或其关键词）去一个外部知识库（如向量数据库、文本数据库）中检索最相关的文档片段或信息块。  
增强 (Augmentation): 将检索到的相关信息与用户的原始问题一起组合成一个新的、更丰富的提示。  
生成 (Generation): 将这个增强后的提示输入给LLM，让LLM基于这些提供的上下文信息来生成回答。  
优势:  
有效缓解模型的“幻觉”问题。  
能够利用最新的、领域特定的或私有的知识，而无需重新训练整个大模型。
提高了答案的可解释性和可追溯性（因为可以指明信息来源）。  
关键组件: 检索器（Retriever）、知识库（Knowledge Base，常使用向量数据库对文本进行嵌入和相似度搜索）、LLM。  
CoT (Chain-of-Thought - 思维链)  
（在Prompt Engineering中已补充）  

Agent (智能体)  
您总结的“决策、感知、行动”是智能体的核心循环。基于LLM的智能体是当前非常前沿和活跃的研究方向。  

LLM作为大脑/规划器: LLM负责理解复杂目标、分析当前情况、制定计划，并决定采取何种行动。  
规划 (Planning):  
LLM可以将用户的复杂指令分解为一系列可执行的子任务。  
常用框架/思想：ReAct (Reason + Act，模型交替进行思考推理和执行动作)、更复杂的规划算法（如任务分解、状态机等）。  
工具使用 (Tool Use): 这是LLM Agent能力大幅扩展的关键。  
LLM可以学习调用外部工具或API来获取它自身不具备的信息或执行特定操作。  
例如：调用搜索引擎获取实时信息、调用计算器进行精确计算、调用代码解释器执行代码、调用数据库查询数据、调用其他特定功能的API（如天气查询、机票预订等）。  
记忆 (Memory):  
短期记忆: LLM自身的上下文窗口。  
长期记忆: 通过外部存储（如向量数据库）来存储和检索过去的交互、学习到的知识或用户偏好，使得Agent能够进行更长时间、更连贯的交互和学习。
感知 (Perception): Agent接收来自环境或工具的反馈信息，更新其对当前状态的理解。  
行动 (Action): Agent根据决策输出具体的操作，可能是调用工具、回复用户，或改变内部状态。  
自我反思/修正 (Self-Reflection/Correction): 更高级的Agent可以评估自己的计划或行动的结果，如果发现问题或未达到预期，可以进行自我修正和调整。  
多智能体系统 (Multi-Agent Systems): 多个Agent协同工作或进行博弈，以完成更复杂的任务。  
可以补充的其他AI应用及相关概念：  

多模态模型 (Multimodal Models):  
能够理解和生成多种类型信息（文本、图像、音频、视频等）的模型，例如OpenAI的GPT-4V(ision)、Google的Gemini。  
它们可以将不同模态的信息进行融合和转换，开启了更丰富的应用场景。
模型评估 (Model Evaluation):  
如何全面评估LLM的能力是一个持续的挑战。除了传统的NLP基准测试（如SuperGLUE, MMLU），还需要关注模型的推理能力、指令遵循能力、创造力、安全性、以及是否存在偏见等。  
人类评估在很多方面仍然是黄金标准。  
伦理考量与负责任的AI (Ethical Considerations & Responsible AI):  
偏见 (Bias): LLM可能从训练数据中学习并放大社会偏见。  
错误信息与滥用 (Misinformation & Misuse): LLM可能被用于生成虚假信息、进行欺诈或恶意攻击。  
透明度与可解释性 (Transparency & Explainability): 理解LLM为何做出特定决策仍然困难。  
知识产权 (Intellectual Property): LLM生成内容的版权归属，以及训练数据可能涉及的版权问题。  
工作替代 (Job Displacement): AI对就业市场的影响。  
开发和部署LLM时，必须高度重视这些伦理问题，并采取负责任的AI原则和措施。  
模型压缩与端侧部署 (Model Compression & On-Device Deployment):  
为了在资源受限的设备（如手机、嵌入式系统）上运行LLM，需要进行模型压缩（如量化、剪枝、知识蒸馏）和优化。  
  
