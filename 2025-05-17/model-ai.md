# 大模型本质——神经网络 #

这是一个很好的开篇，点明了当前大模型技术的核心基石。

1. 机械学习（通过数据训练模型，使系统自动学习并改进任务性能，无需显式编程）：
* 监督学习 (Supervised Learning)：有标签数据，模型学习输入到输出的映射关系。例如：图像分类（输入图片，输出类别标签）、回归预测（输入房屋特征，输出房价）。
* 无监督学习 (Unsupervised Learning)：无标签数据，模型学习数据本身的结构和模式。例如：聚类（将相似的数据点分到一组）、降维（PCA）。
* 强化学习 (Reinforcement Learning)：模型（智能体）通过与环境交互，根据获得的奖励或惩罚来学习最优策略。例如：AlphaGo下棋、机器人控制。

* **深度学习 (Deep Learning)**：是机器学习的一个分支，主要使用包含多个隐藏层的神经网络（即深度神经网络）。它在图像、语音、自然语言处理等领域取得了巨大成功。
* **生成式AI (Generative AI)**：专注于创建新的、原创内容的人工智能，如文本、图像、音频、视频等。大语言模型是生成式AI在文本领域的核心。
* **大语言模型 (Large Language Models, LLMs)**：通常指参数量巨大（数十亿甚至万亿级别）、在海量文本数据上预训练的深度学习模型，展现出强大的自然语言理解和生成能力。
2. 什么是模型：
* 基于输入数据通过假想函数得到输出函数: 这个描述很形象。更形式化地说，模型是一个函数（或一组函数）f_
theta(x)，其中 x 是输入数据，
theta 是模型的参数（权重和偏置），模型的目标是学习一个合适的 
theta，使得 f_
theta(x) 的输出 y 
′
  尽可能接近真实的输出 y（在监督学习中）或揭示数据的某种结构（在无监督学习中）。
* 数据集复杂——>这时候可以使用神经网络: 是的，神经网络尤其擅长处理高维、复杂的非线性数据关系，这是传统机器学习方法可能难以有效处理的。

3. 神经网络基础
* 抽象神经网络：MP模型 (McCulloch-Pitts Neuron Model)：这是最早的、非常简化的神经元模型，它接收二进制输入，并通过一个阈值函数产生二进制输出。它奠定了后续更复杂神经元模型的基础。
* 神经元越来越多，处理难度指数级的增长，如何优化？:
* 硬件层面: GPU、TPU等专用硬件的出现，提供了强大的并行计算能力。
* 算法层面:
* 更高效的优化算法: 如Adam、RMSprop等，不仅仅是梯度下降。
* 更有效的激活函数: 如ReLU及其变种，缓解了梯度消失问题。
* 网络结构创新: 如卷积神经网络 (CNN) 处理图像，循环神经网络 (RNN) 处理序列，以及后来更强大的Transformer。
* 正则化技术: 如Dropout、权重衰减，防止过拟合。
* 分布式训练: 将模型或数据分布到多个设备或机器上进行并行训练。
* 软件框架: TensorFlow, PyTorch等深度学习框架简化了模型构建和训练过程。

4. 模型训练：
* 预测与损失值，梯度优化，反向传播: 这是监督学习中训练神经网络的核心流程。
* 核心流程：定义网络结构，初始化参数，向前传播得到预测值，计算损失，反向传播得到梯度，更新参数。 (您漏了“计算损失”和“更新参数”这两个关键步骤)
* 向前传播 (Forward Propagation)：输入数据通过网络，逐层计算，最终得到模型的预测输出。
* 计算损失 (Loss Calculation)：将模型的预测输出与真实标签（在监督学习中）进行比较，通过损失函数（Loss Function，如交叉熵损失、均方误差损失）计算出两者之间的差异，即损失值。
* 反向传播 (Backward Propagation)：根据损失值，利用链式法则从输出层开始逐层计算每个参数对于损失的梯度（偏导数）。
* 参数更新 (Parameter Update)：使用优化算法（如梯度下降）根据计算出的梯度来调整模型的参数（权重和偏置），以减小损失。

5. 梯度下降：优化算法，还有许多变种
* 是的，梯度下降是最基础的优化算法。
* 常见变种:
* 批量梯度下降 (BGD): 使用整个训练集的梯度来更新参数。计算精确但慢，且对内存要求高。
* 随机梯度下降 (SGD): 每次只用一个样本的梯度来更新参数。速度快，波动大，有助于跳出局部最优，但收敛过程不稳定。
* 小批量随机梯度下降 (Mini-batch SGD): 每次使用一小批样本的梯度来更新参数。是BGD和SGD的折中，兼顾了效率和稳定性，是目前最常用的方法。
* 常见的激活函数：
* Sigmoid: 将输出压缩到 (0, 1)，常用于二分类输出层或早期的神经网络，但易产生梯度消失。
* Tanh: 将输出压缩到 (-1, 1)，通常比Sigmoid表现好一些，但也有梯度消失问题。
* ReLU (Rectified Linear Unit): f(x)=max(0,x)。计算简单，收敛快，有效缓解梯度消失，是目前最常用的激活函数之一。
* Leaky ReLU, PReLU, ELU: ReLU的变种，试图解决ReLU在负数区间的“死亡”问题。
* Softmax: 常用于多分类问题的输出层，将输出转换为概率分布。
* 优化器：避免步长过大或过小:
* 优化器的作用不仅仅是调整步长（学习率），还包括如何利用梯度信息来更有效地更新参数。
* 常见的优化器: SGD with Momentum, AdaGrad, RMSprop, Adam (目前非常流行且效果通常较好)。
* 它们通过引入动量、自适应学习率等机制来加速收敛并改善训练的稳定性。
* 明确位置，找到方向，走，重复步骤直到模型收敛，训练完成: 这个比喻很好地描述了梯度下降的过程。“位置”是当前参数点，“方向”是负梯度方向，“走”是根据学习率更新参数。
* 输入数据来训练，随机梯度下降 全部：批量～ ，小撮：小批量～: 这个对应关系是：
* 全部数据 -> 批量梯度下降 (BGD)
* 单个数据 -> 随机梯度下降 (SGD)
* 小撮数据 -> 小批量随机梯度下降 (Mini-batch SGD)

大模型框架：transformer
* Transformer架构确实是当前大语言模型的基础。
* Tokenizer (分词器)：将原始文本分割成模型可以理解的单元（tokens），这些单元可能是词、子词（subword，如BPE, WordPiece, SentencePiece）。
* Embedding (嵌入层)：将每个token映射到一个高维稠密向量表示。这个向量捕捉了token的语义信息。
* Positional Encoding (位置编码)：由于Transformer的自注意力机制本身不包含序列顺序信息，位置编码向输入嵌入中添加了关于token在序列中位置的信息。
* Transformer Block (Transformer模块)：这是Transformer模型的核心重复单元，通常由以下部分组成：
* Multi-Head Attention (多头注意力机制)：这是关键。
* Add & Norm (残差连接与层归一化)：帮助训练更深的网络。
* Feedforward Network (前馈神经网络)：通常是一个两层的全连接网络，对注意力机制的输出进行进一步处理。
* Add & Norm：再次应用。
* Attention (注意力机制) / Self-Attention (自注意力机制)：允许模型在处理一个token时，动态地关注输入序列中的其他所有token，并根据相关性赋予不同的权重。模型可以“理解”哪些词对当前词的含义最重要。
* QVK (Query, Key, Value) 是自注意力的核心概念:
* Query (查询): 代表当前正在处理的token。
* Key (键): 代表序列中可以被查询的其他token。
* Value (值): 代表序列中其他token实际携带的信息。
* 通过计算Query与所有Key的相似度（通常是点积），得到注意力权重，然后用这些权重对相应的Value进行加权求和，得到当前Query的上下文感知表示。
* Q, K, V 通常是通过将输入嵌入（或上一层的输出）乘以不同的权重矩阵（这些矩阵是模型学习的参数）得到的。
* Feedforward Network (前馈网络)：在注意力层之后，每个位置的输出会独立地通过一个前馈网络。
* Softmax函数: 在多头注意力的最后一步（计算注意力权重时）和模型最终输出层（如果是分类任务，如预测下一个token）会用到，将数值转换为概率分布。

多头注意力模块（自注意力注意模型）qvk是个矩阵: 是的，Q, K, V 通常是矩阵，其中每一行对应序列中的一个token的向量表示。多头注意力允许模型在不同的“表示子空间”中并行地学习不同方面的注意力信息。
transformer变种：
1.  仅编码器/自编码器模型 (Encoder-only / Auto-encoding models): 例如 BERT, RoBERTa。它们通过编码器将输入序列转换为上下文相关的表示。非常适合自然语言理解 (NLU) 任务，如文本分类、情感分析、命名实体识别。它们通常使用“掩码语言模型 (Masked Language Model, MLM)”等目标进行预训练。
2.  仅解码器/自回归模型 (Decoder-only / Autoregressive models): 例如 GPT系列 (GPT-2, GPT-3, GPT-4), PaLM。它们从左到右生成文本，每个token的生成都依赖于之前已经生成的token。非常适合自然语言生成 (NLG) 任务，如文本生成、对话系统、机器翻译（作为其中一部分）。它们通常使用“下一个词预测 (Next Token Prediction)”作为预训练目标。
3.  序列到序列模型/编码器-解码器模型 (Sequence-to-sequence / Encoder-Decoder models): 例如原始Transformer论文中的模型, T5, BART。它们包含一个编码器来处理输入序列和一个解码器来生成输出序列。非常适合机器翻译、文本摘要、问答等需要将一种序列转换为另一种序列的任务。

如何训练一个gpt (这里指GPT系列这类Decoder-only模型)
1.  无监督预训练 (Unsupervised Pre-training):
* 通过大量的文本，进行无监督学习的预训练: 是的，这是核心。使用来自互联网、书籍、文章等海量、多样化的文本数据。
* 模型看见文本，基于上下文预测得到一个token，预测与正确答案损失计算，更新权重: 这是“下一个词预测”或“语言建模”任务。模型接收一段文本作为输入，目标是预测序列中的下一个词（token）。损失函数（通常是交叉熵损失）会比较模型预测的下一个词的概率分布与真实下一个词之间的差异，然后通过反向传播更新模型的权重。
2.  监督微调 (Supervised Fine-Tuning, SFT):
* 改变函数内部函数，适应特定任务: 更准确地说，是使用预训练好的模型权重作为起点，在一个特定任务相关的、带标签的小规模高质量数据集上继续训练模型，以使其更好地适应这个特定任务。例如，对于问答任务，数据集是 (问题, 答案) 对；对于指令遵循任务，数据集是 (指令, 期望的输出) 对。
* 使用少量高质数据，训练，得到sft: “少量”是相对于预训练数据而言，“高质”非常重要。SFT模型是第二阶段的模型。
3.  训练奖励模型 (Reward Model, RM) + 从人类反馈中强化学习 (Reinforcement Learning from Human Feedback, RLHF):
* ai回答，人进行打分，三h原则；，得到奖励模型:
* 首先，使用SFT模型（或多个模型）对一批提示（prompts）生成多个回答。
* 人类评估员对这些回答进行排序或打分，指出哪些回答更好，哪些更差。评估标准通常遵循“3H”原则：Helpful (有用的), Honest (诚实的), Harmless (无害的)。
* 利用这些人类偏好数据（例如，(prompt, chosen_answer, rejected_answer) 对）来训练一个奖励模型 (RM)。RM的目标是学习预测对于给定的prompt和response，人类会给出多高的“奖励分数”。
* 接下来进行强化学习:
* 将SFT模型作为强化学习的策略模型 (policy)。
* 让策略模型针对新的prompt生成回答。
* 使用训练好的奖励模型 (RM) 来评估策略模型生成的回答，并给出奖励分数。
* 使用强化学习算法（如 PPO - Proximal Policy Optimization，这是RLHF中常用的算法）根据奖励模型的反馈来更新策略模型（即SFT模型）的参数，目标是让策略模型生成能够获得更高奖励（即更符合人类偏好）的回答。

llm核心技术
* 预训练：transformer架构和自注意力机制，捕捉文本的依赖关系: 完全正确。
1.  SFT (Supervised Fine-Tuning): 如上所述。
2.  RM (Reward Modeling): 如上所述。
3.  RLHF (Reinforcement Learning from Human Feedback)，PPO: 如上所述。
4.  DPO (Direct Preference Optimization): 一种新兴的、替代传统两阶段RLHF（先训练RM，再用RM做RL）的方法。DPO直接使用人类偏好数据来优化语言模型，试图直接最大化生成更受偏好回答的概率，而无需显式训练一个独立的奖励模型。它被认为可能更简单、更稳定。
5.  PEFT (Parameter-Efficient Fine-Tuning)：参数高效微调。
* 涉及线代: 是的，很多PEFT方法都涉及到对模型权重矩阵的低秩分解、添加少量可训练参数等线性代数操作。
* 目的: 在微调大型预训练模型时，只更新模型参数的一小部分，而不是全部参数。
* 好处:
* 大大减少了微调所需的计算资源和存储空间。
* 可以为多个不同任务训练出多个“轻量级”的适配器 (adapters)，而共享同一个大型基础模型。
* 有时能缓解灾难性遗忘问题。
* 常见方法: LoRA (Low-Rank Adaptation), Prefix Tuning, P-Tuning, Prompt Tuning, Adapters等。

推理时显存计算：模型参数权重80%，输入输出数据，中间激活值，kv cache 等；:  
模型参数权重 (Model Parameters/Weights): 这是占用显存的大头，尤其是对于大模型。例如，一个7B参数的模型，如果用FP16（半精度浮点数，每个参数2字节）存储，就需要大约 14GB 的显存。这通常占显存的绝大部分，您说的80%是可能的。  
输入输出数据 (Input/Output Data): 输入的prompt和生成的文本在转换为token ID并嵌入为向量后，也会占用显存。  
中间激活值 (Intermediate Activations): 在前向传播过程中，每一层网络的输出（激活值）都需要存储起来，以便在反向传播时计算梯度（在训练时）。在推理时，如果不需要反向传播，很多中间激活值可以不保存，但某些结构（如残差连接）可能仍需保留上一层输出。然而，对于自回归生成模型（如GPT），在推理时，主要的中间状态存储是下面的KV Cache。   
KV Cache (Key-Value Cache): 在Transformer解码器（或仅解码器模型）的自注意力层中，当模型逐个生成token时，对于已经生成的token序列，它们的Key (K) 和 Value (V) 向量可以被缓存起来。这样，在生成下一个新token时，新token的Query (Q) 只需要与所有先前token的缓存K, V进行注意力计算，而不需要为之前的每个token重新计算K和V。这大大加速了自回归模型的推理速度，但KV Cache本身会随着生成序列长度的增加而占用越来越多的显存。这是推理时显存占用的一个重要部分，尤其是在生成长文本时。  
补全:  
您最后的“补全”可能是指模型的主要功能之一（文本补全），或者是笔记未完待续的意思。如果是指功能，是的，文本补全（或下一个词预测）是LLM的基础能力。
  
