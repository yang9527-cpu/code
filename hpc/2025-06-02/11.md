## 讲座笔记总结：并行编程实践 (CUDA)

---
### 1. 现代GPU架构 
* **概览**：GPU（图形处理器）设计用于并行处理大量数据。它通过PCI-Express总线与主CPU和系统DRAM连接。GPU自身拥有高速的GDDR内存。
* **NVIDIA Fermi架构示例**：
    * 包含多个GTC（Graphics Thread Cluster）。
    * 拥有多个ROP（Raster Operations Pipeline）和L2缓存。
    * 以NVIDIA Fermi为例，可以拥有多达512个处理元素（Streaming Processors, SPs）。
* **流式多处理器 (SM - Streaming Multiprocessor)**：是GPU的核心组成部分。
    * **Fermi SM**：包含多个核心（Core/SP）、加载/存储单元（LD/ST）、特殊功能单元（SFU）、指令缓存、Warp调度器、分派单元、大量寄存器文件、共享内存/L1缓存以及统一缓存。
    * **新一代GPU (如GeForce RTX 4090)**：
        * 每个SM包含128个核心（SP）。
        * 每个SM拥有16MB共享内存和32MB寄存器文件。
        * 每个SM还有4个用于机器学习的张量核心（Tensor Cores）。
        * 整个芯片可以有多达128个SM，总计16384个核心，512个张量核心，以及72MB的L2缓存。

---
### 2. CUDA执行模型
* **异构计算**：应用程序代码在主机（CPU）和设备（GPU）上协同执行。
    * 串行部分的代码在主机（CPU）上以C语言形式运行。
    * 并行密集型计算部分则作为内核（kernel）代码在设备（GPU）上运行。
* **并行模式**：GPU采用SIMD（单指令多数据）和多线程（具体为SIMT - 单指令多线程）方式执行并行任务，需要细粒度的并行性。
* **内核启动**：主机代码通过特定的语法（如 `KernelA<<< nBlk, nTid >>>(args);`）启动设备上的并行内核。

---
### 3. 向量加法示例
* **传统C代码**：使用一个简单的 `for` 循环来计算向量C中每个元素 `C[i] = A[i] + B[i]`。
* **CUDA主机代码 (vecAdd)** ：
    1.  **内存管理**：
        * 在设备（GPU）上为向量A、B、C分配内存 (`cudaMalloc()`)。
        * 将输入向量A和B从主机内存复制到设备内存 (`cudaMemcpyHostToDevice`)。
    2.  **内核启动**：调用设备上的内核函数执行实际的向量加法。
    3.  **结果回传与清理**：
        * 将结果向量C从设备内存复制回主机内存 (`cudaMemcpyDeviceToHost`)。
        * 释放在设备上分配的内存 (`cudaFree()`)。
* **数据并行性**：
    * 将数据分区，并将计算与数据关联起来。
    * 每个加法操作作为一个独立的任务，由一个线程执行，线程数量等于加法操作的数量，体现了细粒度并行。

---
### 4. 线程组织与调度
* **线程网格 (Grid) 与线程块 (Block)**：
    * 一个CUDA内核由一个线程网格（grid）执行，网格是线程的一维、二维或三维数组。
    * 所有线程运行相同的内核代码（SPMD - Single Program Multiple Data）。
    * 线程网格被划分为多个线程块，这些块被分发到不同的SM上执行。
    * 块内的线程可以通过共享内存、原子操作和屏障同步进行协作。
    * 每个线程拥有索引（如 `blockIdx`, `threadIdx`）用于计算内存地址和控制流程。
* **可扩展性**：
    * 线程块可以以任意相对顺序执行。
    * 硬件可以自由地在任何时间将块分配给任何SM。
    * 这种设计使得内核能够透明地扩展到任意数量的并行处理器。
* **线程块的执行**：
    * 线程以块的粒度分配给SM。
    * 一个SM可以同时执行多个块（例如，Fermi SM最多可处理1536个线程，可以是3个512线程的块或6个256线程的块等）。
    * SM负责管理和调度线程的执行。
* **Warp (线程束)**：
    * 是SM中的基本调度单位，通常由32个线程组成。这是一个实现细节，并非CUDA编程模型的一部分。
    * 一个Warp中的线程以SIMD（单指令多数据）方式执行，即同时执行相同的指令。
    * **问题解答**：如果三个块分配给一个SM，每个块有256个线程，那么：
        * 每个块包含 `256 / 32 = 8` 个Warp。
        * 该SM中总共有 `8 * 3 = 24` 个Warp。
* **线程调度**：SM实现零开销的Warp调度。当一个Warp的下一条指令的操作数准备就绪时，该Warp就有资格执行。

---
### 5. CUDA设备内核代码
* **向量加法内核 (`vecAddKernel`)**：
    ```cuda
    __global__ void vecAddKernel(float* A, float* B, float* C, int n) {
        int i = threadIdx.x + blockDim.x * blockIdx.x; // 计算全局线程索引
        if (i < n) { // 边界检查，确保不越界
            C[i] = A[i] + B[i];
        }
    }
    ```
* **CUDA函数声明限定符**：
    * `__global__`: 定义内核函数，在设备上执行，可从主机调用，必须返回 `void`。
    * `__device__`: 定义设备函数，在设备上执行，仅可从设备调用。
    * `__host__`: 定义主机函数，在主机上执行，仅可从主机调用（若单独使用则此限定符可选）。
    * `__device__` 和 `__host__` 可以一起使用，表示函数既可以在设备上编译/执行，也可以在主机上编译/执行。

---
### 6. CUDA内存模型
* **程序员视角**：
    * **每个线程**：拥有私有的寄存器（Registers）。
    * **每个线程块**：拥有共享内存（Shared Memory），块内线程可见。
    * **每个网格（应用程序级）**：可访问全局内存（Global Memory）、常量内存（Constant Memory）。
    * **主机代码**：可以与设备全局内存进行数据传输。
* **CUDA变量声明与内存、作用域、生命周期** ：
    | 变量声明                      | 内存类型 | 作用域 | 生命周期   |
    | :---------------------------- | :------- | :----- | :--------- |
    | `int LocalVar;` (在核函数内) | 寄存器   | 线程   | 线程       |
    | `__device__ __shared__ int SharedVar;` | 共享内存 | 线程块 | 线程块     |
    | `__device__ int GlobalVar;` (在所有函数外) | 全局内存 | 网格   | 应用程序 |
    | `__device__ __constant__ int ConstantVar;` | 常量内存 | 网格   | 应用程序 |
    * `__device__` 与 `__shared__` 或 `__constant__` 结合使用时是可选的。
* **变量声明位置** ：
    * 寄存器变量和共享内存变量在内核代码内部声明。
    * 全局内存和常量内存变量在所有内核代码外部声明。
    * 主机不能直接访问寄存器或共享内存。
* **设备内存管理API**：
    * `cudaMalloc(void **devPtr, size_t size)`：在设备全局内存中分配对象。
    * `cudaFree(void *devPtr)`：释放设备全局内存中的对象。
    * `cudaMemcpy(void *dst, const void *src, size_t count, enum cudaMemcpyKind kind)`：进行内存数据传输，`kind` 指定传输方向（如 `cudaMemcpyHostToDevice`, `cudaMemcpyDeviceToHost`）。向设备传输通常是异步的。

---
### 7. 内核代码启动与配置
* **启动语法**：`kernelName<<<GridDim, BlockDim>>>(arguments);`。
    * `GridDim`: 网格维度，指定总的线程块数量（可以是一维、二维或三维的 `dim3` 类型）。
    * `BlockDim`: 块维度，指定每个块中的线程数量（可以是一维、二维或三维的 `dim3` 类型）。
* **计算线程块数量**：通常使用向上取整的方式确保所有数据元素都被覆盖。例如，处理 `n` 个元素，每块256个线程，则需要 `ceil(n/256.0)` 个块。这也可以表示为 `(n-1)/256 + 1`。
* **多维数据处理**：`blockIdx` 和 `threadIdx` 可以是多维的，简化了如图像处理等多维数据的内存寻址。
    * 例如，在二维图像处理中，全局行索引 `Row = blockIdx.y * blockDim.y + threadIdx.y;`。
    * 全局列索引 `Col = blockIdx.x * blockDim.x + threadIdx.x;`。
    * 内核启动配置如：`dim3 DimGrid((n-1)/16+1, (m-1)/16+1, 1); dim3 DimBlock(16, 16, 1);` 用于处理 `m x n` 的图像，每块 `16x16` 个线程。

---
### 8. 编译CUDA程序
* CUDA程序是包含C语言和CUDA扩展的集成程序（通常文件后缀为 `.cu`）。
* 使用NVCC编译器（NVIDIA CUDA Compiler）进行编译，例如 `nvcc -o myprog pyprog.cu`。
* NVCC会将主机代码分离出来交由主机C编译器/链接器处理，设备代码则交由设备即时编译器处理，最终在异构计算平台上运行。

---
### 9. 内存性能优化
* **全局内存带宽瓶颈**：频繁访问全局内存（DRAM）会严重限制程序性能，因为其带宽远低于GPU的峰值计算能力。例如，若每次浮点运算都需一次内存访问，则内存带宽可能成为主要瓶颈，导致GPU计算单元空闲。
* **优化策略**：大幅减少内存访问以接近峰值计算性能，主要通过：
    * 内存合并（Memory Coalescing）。
    * 有效利用共享内存。
* **DRAM突发模式 (Bursting)**：
    * 现代DRAM设计为突发模式访问，即一次访问会传输一整个数据块（burst section）。
    * 如果访问不是顺序的，传输的多余字节会被丢弃，造成带宽浪费。
    * 地址空间被划分为多个突发区块（例如128字节或更多）。
* **内存合并 (Memory Coalescing)**：
    * 当一个Warp中的所有线程执行加载指令时，若它们访问的内存位置都落在同一个突发区块内，则硬件只需发起一次DRAM请求，即可满足所有线程的数据需求，这种访问是完全合并的。
    * **判断合并访问**：如果数组访问索引的形式为 `A[(与threadIdx.x无关的表达式) + threadIdx.x]`，则访问通常是合并的。
    * **未合并访问 (Uncoalesced Accesses)**：当Warp中线程访问的内存位置跨越多个突发区块边界时，会发生多次DRAM请求，导致性能下降，部分传输的字节未被使用。
    * 在处理二维数组时，按行主序（C/C++默认）访问（即内层循环或 `threadIdx.x` 对应列索引）通常能实现合并访问。如果按列访问（即内层循环或 `threadIdx.x` 对应行索引），则访问可能不合并。
* **共享内存 (Shared Memory)**：
    * 位于每个SM上的特殊高速内存，其内容由内核代码显式定义和使用。
    * 访问速度远高于全局内存。
    * **作用域**：线程块。块内所有线程共享。
    * **生命周期**：线程块。当块内所有线程执行完毕后，共享内存内容失效。
    * **声明示例**：`__shared__ float ds_in[TILE_WIDTH][TILE_WIDTH];` (在内核函数内部)。
* **切片/分块 (Tiling/Blocking)**：
    * 基本思想：将全局内存中的数据划分为小块（tiles）。
    * 让线程在某一时刻集中处理一个或少数几个数据块，这些块可以被加载到高速的片上内存（如共享内存）。
    * 目的是减少对全局内存的访问次数，重用已加载到片上内存的数据。
* **屏障同步 (Barrier Synchronisation)**：
    * `__syncthreads()`：这是一个块内同步原语。调用它会使块内的所有线程等待，直到所有线程都到达该同步点后，才会继续执行后续指令。
    * 在需要确保共享内存中的数据已被所有线程正确写入或读取后才能进行下一步操作时非常关键（例如，在分块算法中，所有线程加载完数据到共享内存后，需要同步，然后所有线程再从共享内存读取数据进行计算）。

---
