# 第 7 讲 
lecture7 重点介绍了超越简单循环并行化的构造，然后转向分布式内存机器的并行算法设计，并以矩阵乘法作为关键示例。

1. OpenMP 中支持超越 for 循环的通用数据结构 
挑战： 标准的 OpenMP for 循环要求在编译时知道迭代次数，这使得它们不适用于像 while 循环或链表遍历这样迭代次数动态确定的结构。 

（a） Section 指令 (#pragma omp sections)   
用于并行化不同的代码块 (structured_block)，而不是循环迭代。每个 #pragma omp section 由团队中的一个线程执行。必须包含在 #pragma omp parallel 区域内。默认情况下，sections 构造的末尾有一个隐式屏障。可以使用 nowait 子句禁用此隐式屏障。  
示例：   
顺序代码：  
```c
v = alpha();
w = beta();
x = gamma(v, w);
y = delta();
z = zita(x, y);
```
其中 alpha() 和 beta() 可以并行运行 ，gamma(v,w) 和 delta() 可以并行运行。  
使用 sections 并行化：  
```c
#pragma omp parallel
{
    #pragma omp sections
    {
        #pragma omp section
        v = alpha(); [cite: 6]
        #pragma omp section
        w = beta(); [cite: 7]
    } // 此处有隐式屏障

    #pragma omp sections
    {
        #pragma omp section
        x = gamma(v, w); [cite: 7]
        #pragma omp section
        y = delta(); [cite: 7]
    } // 此处有隐式屏障
    z = zita(x, y); [cite: 7] // 此行在第二个 sections 之后由一个线程有效执行，或者如果 zita 需要多个线程的 x 和 y，则需要自行处理。(示例暗示在并行计算 x 和 y 之后 zita 顺序执行)。
}
```
（b） 处理通用数据结构（例如链表）   
while 循环的问题： OpenMP 循环工作共享 (omp for) 不支持 while 循环。   
法一：（不够简洁）：   
遍历列表以计算节点数。将指向每个节点的指针复制到一个数组中。使用 #pragma omp for 并行处理数组元素。  
缺点： 需要对数据进行多次传递。  
法二：（更简洁 - 使用 task 构造）： 

（c） firstprivate 子句   
private(list) 子句为每个线程创建一个局部的、未初始化的变量副本。   
示例显示 tmp 在每个线程的私有副本中未初始化 ：  
```c
int tmp = 0; 
#pragma omp parallel for private(tmp)
for (int i=0; i<n; i++) {
    tmp += i; 
}
printf("%d\n", tmp); 
```
firstprivate(list) 子句为每个线程创建一个局部副本，并使用主线程中原始共享变量的值对其进行初始化 。 
示例：
```c
int tmp = 0; [cite: 18]
#pragma omp parallel for firstprivate(tmp) [cite: 19]
for (int i=0; i<n; i++) {
    tmp += i; 
}
printf("%d\n", tmp); 
```
（d）Task 构造 (#pragma omp task)   
任务是独立的工作单元，由代码和数据环境组成。  
当线程遇到 task 指令时，它会“打包”任务（代码和数据）。  
运行时系统将这些任务排队，并将其分配给可用的线程以供稍后执行。  
示例（链表遍历）：   
```C
#pragma omp parallel // 1. 创建一个线程团队
{
    #pragma omp single // 2. 一个线程执行此块
    {
        node *p = head; // 3. 这个单线程迭代并创建
        while (p) {
            #pragma omp task firstprivate(p) // p 需要是 firstprivate 以捕获其当前值给每个任务 
            process(p); 
            p = p->next; 
        }
    } // single 末尾的隐式屏障：其他线程在此等待
} // 线程从任务队列中执行任务。一旦所有任务完成，执行将移出并行区域。
```
使用 task 和 single 的执行流程：   
创建一个并行区域。  
一个线程（由于 omp single）遇到 while 循环并开始创建任务。  
团队中的其他线程最初可能处于空闲状态，或在 single 构造的隐式屏障处等待。  
当任务生成时，空闲线程（可能包括生成任务的线程，在它完成生成后或遇到调度点时）可以从任务队列中获取它们并执行它们。  
所有任务必须在 single 构造末尾的隐式屏障（如果任何任务与其绑定）或并行区域结束之前完成。（图表显示等待的线程正在执行任务，这意味着在移出 single 的作用域之前完成）。  
2. 共享内存 OpenMP 编程总结   
共享内存机器特性：   
全局数据由所有线程共享。数据在不同线程使用时无需移动 。
任务划分和分配相对于分布式内存机器更简单。必须通过对共享变量的同步来显式协调线程。最小化同步开销非常重要。  
数据局部性（优化内存层次结构使用，增加计算强度）对于单处理器/核心的性能也非常重要。  
OpenMP 概述：  
一个基于指令的应用程序编程接口 (API)，用于在共享内存架构上开发并行程序。  
它是一个小型 API，通过更简单的指令隐藏了繁琐的线程调用。  
允许程序员将程序分为串行区域和并行区域，而不是显式创建并发执行的线程。  
提供一些同步构造。  
关键 OpenMP 指令、子句和函数：  
线程创建： #pragma omp parallel   
工作共享： #pragma omp for、#pragma omp single、#pragma omp sections、#pragma omp task   
同步（防止数据竞争）： #pragma omp critical、#pragma omp atomic、#pragma omp barrier   
数据环境子句： private(list)、firstprivate(list)、shared(list)、reduction(op:list)   
循环调度子句： schedule(static, chunk)、schedule(dynamic, chunk)、collapse(n)   
禁用隐式屏障： nowait   
环境/运行时函数： omp_set_num_threads()、omp_get_num_threads()、omp_get_thread_num()、omp_get_procs()、omp_get_wtime()   
重要注意事项（OpenMP 不会自动执行的操作）：   
自动并行化代码。保证加速。提供免于数据竞争的自由（这是程序员的责任）。  
编写 OpenMP 程序的步骤：   
设计并行算法。相应地编写顺序程序。优化顺序程序。添加必要的 omp 指令、子句和函数。测试和调整性能。  
如有必要，重复上述步骤，也可能尝试其他可能的算法。  
3. 分布式内存机器的并行算法设计   
分布式内存平台特性：   
由一组处理单元（或计算节点）组成。每个处理单元都有其自己的（独占）内存，其他处理单元无法访问（进程之间没有共享变量）。处理单元使用（各种）发送和接收原语显式通信。流行的库（如 MPI 和 PVM）为进程通信提供此类原语。  
与共享内存编程的比较：   
共享内存： 由于全局数据访问，任务分配相对简单。  
分布式内存：  
数据必须显式分区、分发并保持在每个进程的本地。工作的分解影响工作的分配。  
对于阻塞式发送和接收（进程被阻塞直到数据接收完毕），一对发送/接收操作充当同步点，因此同步是隐式的。  
可能会觉得为分布式内存平台编写并行程序比为共享内存平台难得多。  
如今，计算机集群通常具有多个计算节点，每个节点都是一个多核处理器，这需要混合编程：在集群的多核节点内使用共享内存编程，在节点之间使用消息传递。  
分布式内存的关键设计考量：   
数据如何分区和分发。任务（或工作）如何分配，这也与数据分区有关。如何最小化通信开销。数据局部性（以减少通信开销）。当然，还有如何平衡工作负载。  
4. 分布式内存上的并行矩阵乘法算法   
任务依赖图： 计算每个 c(ij)的数据依赖关系表明，所有输出元素 c(ij)都可以无任何依赖地同时计算。  
分区和分配：   
在共享内存平台的算法设计中，讨论了如何划分矩阵 C，然后将输入数据与计算关联起来 。输入矩阵是共享的，不需要在主内存中移动。  
对于分布式内存机器，输入数据矩阵 A 和 B 也需要被划分并分发到不同的进程。  
关键问题：如何划分任务和数据，以及如何将它们分配给进程，更重要的是，如何在进程之间移动数据以确保正确的数据在正确的时间到达正确的进程，并且通信开销也最小化。  
任务和数据关联示例：   
1D 分块：C 的一个块行（任务）与 A 的一个块行和整个 B 相关联。  
2D 分块：C 的一个块（任务）与 A 的一个块行和 B 的一个块列相关联。  
（a） 1D 阵列/环上的并行矩阵乘法   
块矩阵回顾：将矩阵 C 和 A 划分为 p×1 的块矩阵，每个块由 n/p 行组成 。令 C(i) 和 A(i) 表示大小为 n/p×n 的每个块（称为块行）。我们有 C(i)=C(i)+A(i)×B。  
进一步划分 A(i)：A(i,j) 是 A(i) 的 n/p×n/p 子块。并将 B 划分为 p×1 的块矩阵。于是有 C(i)=C(i)+∑j A(i,j)×B(j)。
数据分布： 假设我们有 p 个处理器，组织为 1D 阵列/环 。我们将每个矩阵（A、B 和 C）划分为行块矩阵，并将每个块行 (A(i)、B(i) 和 C(i)) 分配给一个处理器。  
通信挑战： 为了计算每个 C(i)，我们需要 A(i) 和整个 B 。即 C(i)=C(i)+A(i)×B=C(i)+∑j A(i,j)×B(j)。然而，B(j) 分布在不同的处理器上，我们需要显式通信。  
通信策略：  
广播：在阶段 i，p 个处理器广播 B(i) 。注意：一次广播需要 logp 次发送/接收步骤。  
移位（更好）：在每个阶段，进程并行地向邻居发送/接收 B(i)。图示显示了一种 B 块的循环移位模式在初始状态下，Pi 拥有 A(i,0),A(i,1),…,A(i,p−1) 和 B(i) 。在每个阶段 k，Pi 计算 A(i,(i+k)(modp))×B((i+k)(modp))。然后，每个 B 块被移位到下一个处理器。
分析（1D 算法）：   
矩阵被划分为块行 (n/p×n)。在一个并行步骤（或阶段）中，每个进程：  
发送一个块行 (B(j)) 并接收一个块行 (∼n^2/p)。A(i,j) (n/p×n/p) 和 B(j) (n/p×n) 的乘法运算次数为 ∼n^3/p2   
计算强度：∼n/p。  
并行步骤的数量是 p。  
（b）2D 网格/环面上的并行矩阵乘法   
使用 2D 分区，并假设 p= sqrt(p)*sqrt(p)，即 p 个进程组织为 2D 网格/环面。令 C(i,j) 表示大小为 n/sqrt（p）×n/sqrt（p）的子矩阵，对于 A(i,j) 和 B(i,j) 类似。每个 C(i,j) 需要 A(i,k) 的一个块行和 B(k,j) 的一个块列，这些数据由不同的进程持有。  
通信：水平移动 A(i,j)，垂直移动 B(i,j)。  
算法 1（广播）：   
对于 P（ij）计算 C(i,j)=∑k A(i,k)B(k,j)。在第 k 轮 (k 从 0 到 p−1)：  
持有 A(i,k) 的处理器将其沿其所在的处理器行 i 广播。  
持有 B(k,j) 的处理器将其沿其所在的处理器列 j 广播。  
每个 P（ij）计算 Temp=A(i,k)×B(k,j) 并累加到 C(i,j)。  
算法 2（移位 - Cannon 算法变体）：    
初始对齐： A(i,j) 左移 i 次， B(i,j) 上移 j 次 。
计算与移位（重复 p 次）：  
每个处理器 P（ij）计算 C(i,j)=C(i,j)+A（current）×B（current）。  
将 A 块左移 1 位。  
将 B 块上移 1 位。  
分析（2D 算法）：   
矩阵被划分为多个子矩阵 (n/sqrt（p）×n/sqrt（p）)。  
在一个并行步骤中，每个进程：  
水平发送一个子矩阵 A(i,j) 并接收一个子矩阵 (∼n^2/p)。  
垂直发送一个子矩阵 B(i,j) 并接收一个子矩阵 (∼n^2/p)。  
A(i,k) 和 B(k,j) 的乘法运算次数为 ∼n^3/(psqrt(p))。  
计算强度：∼n/sqrt(p)。
并行步骤的数量是 sqrt(p)。  
（c） 比较：1D 与 2D 算法 
问题： 哪一个性能更好？   
答案： 2D 并行算法更好。  
原因：   
尽管 2D 算法在一个并行步骤中需要移动两倍的数据量（同时移动 A(i,j) 和 B(i,j)），但其操作次数增加了p倍。计算强度增加了p倍。由于给定问题的总工作量是固定的，并行步骤的数量也减少了p倍。这极大地减少了通信开销。  
5. 
通过添加展开因子为 4 的循环展开，优化的 OpenMP 程序以计算矩阵乘法 C=AA 
